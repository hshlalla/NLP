{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.어간 추출과 표제어 추출.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshlalla/NLP/blob/main/2_%EC%96%B4%EA%B0%84_%EC%B6%94%EC%B6%9C%EA%B3%BC_%ED%91%9C%EC%A0%9C%EC%96%B4_%EC%B6%94%EC%B6%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTpT9J3CE8Fq"
      },
      "source": [
        "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 어간 추출과 표제어 추출 튜토리얼입니다.  \n",
        "\n",
        "링크 : https://wikidocs.net/21707"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBWskO_4EjCh",
        "outputId": "285a035b-cbb2-4c73-ed55-9d5324223e05"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J5cLbpkEuW7",
        "outputId": "3a4c501c-329c-439e-d4c9-98723f081ddb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YolDZsvREntt",
        "outputId": "310ec21e-d703-4989-8004-9b23f0317f2a"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([lemmatizer.lemmatize(w) for w in words])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9te_ttZEpMs",
        "outputId": "ce5f586d-6c8f-490d-aa11-d681632fca8b"
      },
      "source": [
        "print(lemmatizer.lemmatize('dies', 'v'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9hP-XRTEqLs",
        "outputId": "fd83d9d1-cabf-441b-829c-0f1f0d35c82a"
      },
      "source": [
        "print(lemmatizer.lemmatize('watched', 'v'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK6NxzYyEq3c",
        "outputId": "58e835fb-e21c-45f9-b2d1-d953eadac496"
      },
      "source": [
        "print(lemmatizer.lemmatize('has', 'v'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gJmj2wjNb-ee",
        "outputId": "b30aee2e-ab5f-4a1f-cfb5-fe76a19d9c4b"
      },
      "source": [
        "lemmatizer.lemmatize(\"has\",\"v\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRURBaL3EymE",
        "outputId": "3317279a-4bde-4d44-d3af-4a5c1b974c9c"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhjMsNBpEz7c",
        "outputId": "56b6ea55-3146-439a-ded2-7a65d0cdca30"
      },
      "source": [
        "print([stemmer.stem(w) for w in words])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIeElNsgdgdU"
      },
      "source": [
        "# 포터 알고리즘\n",
        "가령, 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다.\n",
        "* ALIZE → AL\n",
        "* ANCE → 제거\n",
        "* ICAL → IC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M5cviG2E2uE",
        "outputId": "d444daec-cfca-40f1-88fd-fea4f23db84a"
      },
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([stemmer.stem(w) for w in words])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VFiuKj-E3os",
        "outputId": "1e3c2340-7f2d-499a-afa9-049927e66e02"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([stemmer.stem(w) for w in words])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-25ghXWE4j0",
        "outputId": "08ba368f-1afb-4326-a7c4-9b399e70c4ca"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([lancaster_stemmer.stem(w) for w in words])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2zZlCKpe17-"
      },
      "source": [
        "Stemming\n",
        "* am → am\n",
        "* the going → the go\n",
        "* having → hav\n",
        "\n",
        "Lemmatization\n",
        "* am → be\n",
        "* the going → the going\n",
        "* having → have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fuj5COpZe6QT"
      },
      "source": [
        "# 3. 한국어에서의 어간 추출\n",
        "\n",
        "한국어에 대해서는 별도 실습은 진행하지 않지만, 한국어의 어간에 대해서 간략히 설명합니다. 한국어는 아래의 표와 같이 5언 9품사의 구조를 가지고 있습니다.\n",
        "\n",
        "* 언\t   품사\n",
        "* 체언     명사, 대명사, 수사\n",
        "* 수식언 \t관형사, 부사\n",
        "* 관계언\t조사\n",
        "* 독립언\t감탄사\n",
        "* 용언     동사, 형용사\n",
        "\n",
        "이 중 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성됩니다. 앞으로 용언이라고 언급하는 부분은 전부 동사와 형용사를 포함하여 언급하는 개념입니다.\n",
        "\n",
        "# (1) 활용(conjugation)\n",
        "\n",
        "- 활용(conjugation)은 한국어에서만 가지는 특징이 아니라, 인도유럽어(indo-european language)에서도 주로 볼 수 있는 언어적 특징 중 하나를 말하는 통칭적인 개념입니다. 다만, 여기서는 한국어에 한정하여 설명합니다.\n",
        "\n",
        "- 활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말합니다.\n",
        "\n",
        "- 어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).\n",
        "\n",
        "- 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행\n",
        "\n",
        "- 활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉩니다.\n",
        "\n",
        "# (2) 규칙 활용\n",
        "\n",
        "- 규칙 활용은 어간이 어미를 취할 때, 어간의 모습이 일정합니다. 아래의 예제는 어간과 어미가 합쳐질 때, 어간의 형태가 바뀌지 않음을 보여줍니다.\n",
        "\n",
        "-  잡/어간 + 다/어미\n",
        "이 경우에는 어간이 어미가 붙기전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 됩니다.\n",
        "\n",
        "# (3) 불규칙 활용\n",
        "\n",
        "- 불규칙 활용은 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우를 말합니다.\n",
        "\n",
        "- 예를 들어 ‘듣-, 돕-, 곱-, 잇-, 오르-, 노랗-’ 등이 ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’와 같이 어간의 형식이 달라지는 일이 있거나 ‘오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러’와 같이 일반적인 어미가 아닌 특수한 어미를 취하는 경우 불규칙활용을 하는 예에 속합니다.\n",
        "\n",
        "- 이 경우에는 어간이 어미가 붙는 과정에서 어간의 모습이 바뀌었으므로 단순한 분리만으로 어간 추출이 되지 않고 좀 더 복잡한 규칙을 필요로 합니다.\n",
        "\n",
        "아래의 링크는 다양한 불규칙 활용의 예를 보여줍니다.\n",
        "링크 : https://namu.wiki/w/한국어/불규칙%20활용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSZguIpEfFEA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}